#load "core/std/wasi"

#load_path "src"
#load "neuralnet"

use package core

MNIST_DataLoader :: struct {
    use base : DataLoader(MNIST_Sample);
    
    images, labels : io.FileStream;
    
    make :: (image_path := "data/train-images-idx3-ubyte", label_path := "data/train-labels-idx1-ubyte") -> MNIST_DataLoader {
        mnist_data: MNIST_DataLoader;
        mnist_data.vtable = ^mnist_dataloader_functions;
        
        err : io.Error;
        err, mnist_data.images = io.open(image_path);
        assert(err == io.Error.None, "There was an error loading the image file");
        
        err, mnist_data.labels = io.open(label_path);
        assert(err == io.Error.None, "There was an error loading the label file");

        return mnist_data;
    }

    close :: (use mnist_data: ^MNIST_DataLoader) {
        io.stream_close(^images);
        io.stream_close(^labels);
    }
    
    get_count :: (use data: ^MNIST_DataLoader) -> u32 {
        return 60000;
    }
    
    get_item :: (use data: ^MNIST_DataLoader, index: u32, use sample: ^MNIST_Sample) -> bool {
        assert(input.count  == 28 * 28, "Input slice was of wrong size. Expected 784.");
        assert(output.count == 10,      "Output slice was of wrong size. Expected 10.");
        
        if index > 60000 do return false;
        
        location := 16 + index * 784;
        input_tmp : [784] u8;
        _, bytes_read := io.stream_read_at(^images, location, ~~ input_tmp);
        
        location = 8 + index;
        label_buf : [1] u8;
        _, bytes_read = io.stream_read_at(^labels, location, ~~ label_buf);
        
        // CLEANUP: The double cast that is necessary here is gross.
        for i: input.count do input[i] = (cast(f32) cast(u32) input_tmp[i]) / 255;
        
        for i: output.count do output[i] = 0.0f;
        output[cast(u32) label_buf[0]] = 1.0f;
        
        return true;
    }
}

mnist_dataloader_functions := <DataLoader_Functions(MNIST_Sample)>.{
    get_count = MNIST_DataLoader.get_count,
    get_item  = MNIST_DataLoader.get_item,
}

MNIST_Sample :: struct {
    input, output : [] f32;
    
    init :: (use s: ^MNIST_Sample, allocator := context.allocator) {
        input = memory.make_slice(f32, 784, allocator);
        output = memory.make_slice(f32, 10, allocator);
    }

    deinit :: (use s: ^MNIST_Sample, allocator := context.allocator) {
        raw_free(allocator, input.data);
        raw_free(allocator, output.data);
    }
}

train :: (
    nn: ^NeuralNet,                               // The neural network.
    dataloader: ^DataLoader($Sample_Type),        // Data loader that provides samples of type Sample_Type.
    optimizer: ^Optimizer,                        // The optimizer of choice that is expected to have neural net parameters initialized.
    criterion: Criterion = mean_squared_error,    // The criterion of choice.
    batch_size := 10,                             // How many samples per batch.
    batches_per_epoch := -1,                      // -1 means dataset size divided by batch size
    epochs := 5,                                  // The number of epochs
    ) {

    sample : Sample_Type;
    sample->init();
    defer sample->deinit();

    training_example_count := dataloader_get_count(dataloader);
    printf("Training sample count: %i\n", training_example_count);

    if batches_per_epoch == -1 {
        batches_per_epoch = training_example_count / batch_size;
    }
    
    // Tracking how many of the past 100 samples were correct.
    past_100_correct := 0;

    for epoch: epochs {
        printf("Staring epoch %i ===================================\n", epoch + 1);

        for batch_num: batches_per_epoch {
            optimizer_zero_gradient(optimizer);

            for batch: batch_size {
                sample_num := random.between(0, training_example_count);
                dataloader_get_item(dataloader, sample_num, ^sample);

                (*nn)->forward(sample.input);
                (*nn)->backward(sample.output, criterion);

                label, _   := array.greatest(sample.output);
                prediction := (*nn)->get_prediction();
                if prediction == label do past_100_correct += 1;
            }

            optimizer_step(optimizer);

            if batch_num % (100 / batch_size) == 0 {
                loss := (*nn)->get_loss(sample.output, criterion);
                printf("Loss: %f         Correct: %i / 100\n", cast(f32) loss, past_100_correct);

                past_100_correct = 0;
            }
        }
    }
}

main :: (args: [] cstr) {
    nn := NeuralNet.make(28 * 28, 512, 256, 100, 10);
    defer nn->free();

    random.set_seed(5234);

    mnist_data := MNIST_DataLoader.make();
    defer mnist_data->close();

    optimizer := sgd_optimizer_create(^nn, learning_rate = 0.01f);
    nn->supply_parameters(^optimizer);

    train(^nn, ^mnist_data.base, ^optimizer);
}




// Old code for printing the outputs fancily:
/*
            {
                print_colored_array :: (arr: [] $T, color_idx: i32, color_code := 94) {
                    for i: arr.count {
                        if i == color_idx {
                            printf("\x1b[%im", color_code);
                            print(arr[i]);
                            print("\x1b[0m ");
                        } else {
                            print(arr[i]);
                            print(" ");
                        }
                    }
                    print("\n");
                }

                color := 94;
                if prediction != label do color = 91;

                output := (*nn)->get_output();

                print_colored_array(sample.output, label, color);
                print_colored_array(output, prediction, color);

                loss := (*nn)->get_loss(sample.output, criterion);
                printf("Loss: %f         Correct: %i / 100\n", cast(f32) loss, past_100_correct);

                past_100_correct = 0;
                
                // if ex % 10000 == 0 {
                //     println("Saving neural network...");
                //     neural_net_save(nn, "data/still_working.nn");
                // }
            }
            */
