#load "core/std/wasi"

#load_path "src"
#load "neuralnet"

use package core

MNIST_DataLoader :: struct {
    use base : DataLoader(MNIST_Sample);
    
    images : io.FileStream;
    labels : io.FileStream;
    
    make :: (image_path := "data/train-images-idx3-ubyte", label_path := "data/train-labels-idx1-ubyte") -> MNIST_DataLoader {
        mnist_data: MNIST_DataLoader;
        mnist_data.vtable = ^mnist_dataloader_functions;
        
        err : io.Error;
        err, mnist_data.images = io.open(image_path);
        assert(err == io.Error.None, "There was an error loading the image file");
        
        err, mnist_data.labels = io.open(label_path);
        assert(err == io.Error.None, "There was an error loading the label file");

        return mnist_data;
    }
       
    close :: (use mnist_data: ^MNIST_DataLoader) {
        io.stream_close(^images);
        io.stream_close(^labels);
    }
    
    get_count :: (use data: ^MNIST_DataLoader) -> u32 {
        return 50000;
    }
    
    get_item :: (use data: ^MNIST_DataLoader, index: u32, use sample: ^MNIST_Sample) -> bool {
        assert(input.count  == 28 * 28, "Input slice was of wrong size. Expected 784.");
        assert(output.count == 10,      "Output slice was of wrong size. Expected 10.");
        
        if index > 50000 do return false;
        
        location := 16 + index * 784;
        input_tmp : [784] u8;
        _, bytes_read := io.stream_read_at(^images, location, ~~ input_tmp);
        
        location = 8 + index;
        label_buf : [1] u8;
        _, bytes_read = io.stream_read_at(^labels, location, ~~ label_buf);
        
        // CLEANUP: The double cast that is necessary here is gross.
        for i: input.count do input[i] = (cast(f32) cast(u32) input_tmp[i]) / 255;
        
        for i: output.count do output[i] = 0.0f;
        output[cast(u32) label_buf[0]] = 1.0f;
        
        return true;
    }
}

MNIST_Sample :: struct {
    // NOTE(Brendan Hansen): Expected to be 28 * 28 elements in size
    input : [] f32;
    
    // NOTE(Brendan Hansen): Expected to be 10 elements in size
    output : [] f32;
}

mnist_dataloader_functions := <DataLoader_Functions(MNIST_Sample)>.{
    get_count = MNIST_DataLoader.get_count,
    get_item  = MNIST_DataLoader.get_item,
}

// TODO(Brendan Hansen): Generalize this to all data types 
train :: (nn: ^NeuralNet, dataloader: ^DataLoader(MNIST_Sample), optimizer: ^Optimizer, criterion: Criterion = mean_squared_error) {
    sample : MNIST_Sample;
    sample.input  = memory.make_slice(f32, 784);
    sample.output = memory.make_slice(f32, 10);
    defer cfree(sample.input.data);
    defer cfree(sample.output.data);

    training_example_count := dataloader_get_count(dataloader);
    
    past_100_correct := 0;
    for i: 10 {
        printf("Staring epoch %i ===================================\n", i);
        for ex: training_example_count {
            dataloader_get_item(dataloader, ex, ^sample);
            
            optimizer_zero_gradient(optimizer);
            neural_net_forward(nn, ~~ sample.input);
            neural_net_backward(nn, ~~ sample.output, criterion);
            optimizer_step(optimizer);


            // NOTE(Brendan Hansen): Prediction printing and tracking.
            label, _   := array.greatest(sample.output);
            prediction := neural_net_get_prediction(nn);
            if prediction == label do past_100_correct += 1;

            if ex % 100 == 0 {
                print_colored_array :: (arr: [] $T, color_idx: i32, color_code := 94) {
                    for i: arr.count {
                        if i == color_idx {
                            printf("\x1b[%im", color_code);
                            print(arr[i]);
                            print("\x1b[0m ");
                        } else {
                            print(arr[i]);
                            print(" ");
                        }
                    }
                    print("\n");
                }

                color := 94;
                if prediction != label do color = 91;

                output := neural_net_get_output(nn);

                print_colored_array(sample.output, label, color);
                print_colored_array(output, prediction, color);

                loss := neural_net_loss(nn, sample.output, criterion);
                printf("Loss: %f         Correct: %i / 100\n", cast(f32) loss, past_100_correct);

                past_100_correct = 0;
                
                if ex % 10000 == 0 {
                    println("Saving neural network...");
                    neural_net_save(nn, "data/still_working.nn");
                }
            }
        }
    }
}

main :: (args: [] cstr) {
    // Enables a logging allocator to print every allocation
    // main_allocator := context.allocator;
    // context.allocator = alloc.log.logging_allocator(^main_allocator);

    nn := make_neural_net(28 * 28, 512, 256, 100, 10);
    defer neural_net_free(^nn);

    random.set_seed(5234);

    mnist_data := MNIST_DataLoader.make();
    defer mnist_data.close(^mnist_data);

    optimizer := sgd_optimizer_create(^nn, learning_rate = 0.005f);
    neural_net_supply_parameters(^nn, ^optimizer);

    println("Starting training");
    train(^nn, ^mnist_data, ^optimizer);
}