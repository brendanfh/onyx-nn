use package core

// To easily change to 64-bit floats if needed.
float :: #type f32;


NeuralNet :: struct {
    layers  : [] Layer;

    // CLEANUP: Move these to core.alloc, so the nesting isn't nearly as terrible.
    layer_arena : alloc.arena.ArenaState;
}

make_neural_net :: (layer_sizes: ..i32) -> NeuralNet {
    net : NeuralNet;

    net.layer_arena  = alloc.arena.make(context.allocator, 64 * 1024 * 1024); // 64 MiB
    layer_allocator := alloc.arena.make_allocator(^net.layer_arena);

    net.layers = memory.make_slice(Layer, layer_sizes.count, allocator = layer_allocator);

    init_layer(^net.layers[0], layer_sizes[0], 0, allocator = layer_allocator);
    for i: 1 .. net.layers.count {
        init_layer(^net.layers[i], layer_sizes[i], layer_sizes[i - 1], allocator = layer_allocator);
    }

    return net;
}

neural_net_free :: (use nn: ^NeuralNet) {
    alloc.arena.free(^layer_arena);
}

neural_net_forward :: (use nn: ^NeuralNet, input: [] float) {
    assert(input.count == layers[0].neurons.count, "Input does not have the same size as the first layer.");

    for i: input.count do layers[0].neurons[i] = input[i];

    for i: 1 .. layers.count {
        layer_forward(^layers[i], ^layers[i - 1]);
    }
}

neural_net_backward :: (use nn: ^NeuralNet, expected_output: [] float) {

}

neural_net_get_output :: (use nn: ^NeuralNet) -> [] float {
    return layers[layers.count - 1].neurons;
}

neural_net_loss :: (use nn: ^NeuralNet, expected_output: [] float) -> float {
    // MSE loss
    assert(layers[layers.count - 1].neurons.count == expected_output.count,
            "Expected output does not have the same size as the last layer.");

    output := layers[layers.count - 1].neurons;

    squared_sum: float = 0;
    for i: expected_output.count {
        diff := output[i] - expected_output[i];
        squared_sum += diff * diff;
    }

    loss := math.sqrt(squared_sum);
    return loss;
}


Layer :: struct {
    neurons :   [] float;
    weights : [][] float; // CLEANUP: Make this a rank 1 slice
}

init_layer :: (use layer: ^Layer, layer_size: u32, prev_layer_size: u32, allocator := context.allocator) {
    neurons = memory.make_slice(float, layer_size, allocator);

    if prev_layer_size > 0 {
        weights = memory.make_slice(#type [] float, layer_size, allocator);

        for ^weight: weights {
            *weight = memory.make_slice(float, prev_layer_size, allocator);
        }

        randomize_weights(layer);
    }
}

randomize_weights :: (use layer: ^Layer) {
    for ^weight: weights {
        for ^w: *weight {
            *w = random.float(-1.0f, 1.0f);
        }
    }
}

layer_forward :: (use layer: ^Layer, prev_layer: ^Layer) {
    for i: neurons.count {
        neurons[i] = 0;
        for j: weights[i].count {
            neurons[i] += prev_layer.neurons[j] * weights[i][j];
        }

        neurons[i] = sigmoid(neurons[i]);
    }
}




sigmoid :: (x: float) -> float {
    ex := math.exp(x);
    return ex / (1 + ex);
}

