use package core

// To easily change to 64-bit floats if needed.
float :: #type f32;


NeuralNet :: struct {
    layers  : [] Layer;

    // CLEANUP: Move these to core.alloc, so the nesting isn't nearly as terrible.
    layer_arena : alloc.arena.ArenaState;
}

make_neural_net :: (layer_sizes: ..i32) -> NeuralNet {
    net : NeuralNet;

    net.layer_arena  = alloc.arena.make(context.allocator, 64 * 1024 * 1024); // 64 MiB
    layer_allocator := alloc.arena.make_allocator(^net.layer_arena);

    net.layers = memory.make_slice(Layer, layer_sizes.count, allocator = layer_allocator);

    init_layer(^net.layers[0], layer_sizes[0], 0, allocator = layer_allocator);
    for i: 1 .. net.layers.count {
        init_layer(^net.layers[i], layer_sizes[i], layer_sizes[i - 1], allocator = layer_allocator);
    }

    return net;
}

neural_net_free :: (use nn: ^NeuralNet) {
    alloc.arena.free(^layer_arena);
}

neural_net_forward :: (use nn: ^NeuralNet, input: [] float) {
    assert(input.count == layers[0].neurons.count, "Input does not have the same size as the first layer.");

    for i: input.count do layers[0].neurons[i] = input[i];

    for i: 1 .. layers.count {
        layer_forward(^layers[i], ^layers[i - 1]);
    }
}

neural_net_backward :: (use nn: ^NeuralNet, expected_output: [] float) {
    assert(layers[layers.count - 1].neurons.count == expected_output.count,
            "Expected output does not have the same size as the last layer.");

    LEARNING_RATE :: cast(float) 0.01;

    while i := layers.count - 1; i >= 1 {
        defer i -= 1;

        for j: layers[i].neurons.count {
            sigmoid_value   := layers[i].neurons[j];
            d_sigmoid_value := sigmoid_value * (1 - sigmoid_value);

            if i == layers.count - 1 {
                layers[i].deltas[j] = 2 * (expected_output[j] - sigmoid_value) * d_sigmoid_value;
            } else {
                d_neuron: float = 0;
                for k: layers[i + 1].neurons.count {
                    d_neuron += layers[i + 1].deltas[k] * layers[i + 1].weights[k][j];
                }
                layers[i].deltas[j] = d_neuron * d_sigmoid_value;
            }
        }
    }

    for i: 1 .. layers.count {
        for j: layers[i].neurons.count {
            layers[i].biases[j] += LEARNING_RATE * layers[i].deltas[j];

            for k: layers[i].weights[j].count {
                layers[i].weights[j][k] += LEARNING_RATE * layers[i].deltas[j] * layers[i].neurons[k];
            }
        }
    }
}

neural_net_get_output :: (use nn: ^NeuralNet) -> [] float {
    return layers[layers.count - 1].neurons;
}

neural_net_loss :: (use nn: ^NeuralNet, expected_output: [] float) -> float {
    // MSE loss
    assert(layers[layers.count - 1].neurons.count == expected_output.count,
            "Expected output does not have the same size as the last layer.");

    output := layers[layers.count - 1].neurons;

    squared_sum: float = 0;
    for i: expected_output.count {
        diff := output[i] - expected_output[i];
        squared_sum += diff * diff;
    }

    loss := squared_sum / ~~expected_output.count;
    return loss;
}


Layer :: struct {
    neurons :   [] float;
    biases  :   [] float;
    weights : [][] float; // CLEANUP: Make this a rank 1 slice
    deltas  :   [] float;
}

init_layer :: (use layer: ^Layer, layer_size: u32, prev_layer_size: u32, allocator := context.allocator) {
    neurons = memory.make_slice(float, layer_size, allocator);
    deltas  = memory.make_slice(float, layer_size, allocator);
    biases  = memory.make_slice(float, layer_size, allocator);

    if prev_layer_size > 0 {
        weights = memory.make_slice(#type [] float, layer_size, allocator);

        for ^weight: weights {
            *weight = memory.make_slice(float, prev_layer_size, allocator);
        }

        randomize_weights_and_biases(layer);
    }
}

randomize_weights_and_biases :: (use layer: ^Layer) {
    for ^weight: weights {
        for ^w: *weight {
            *w = random.float(-0.5f, -0.5f);
        }
    }

    for ^bias: biases do *bias = random.float(-0.5f, 0.5f);
}

layer_forward :: (use layer: ^Layer, prev_layer: ^Layer) {
    for i: neurons.count {
        neurons[i] = biases[i];
        for j: weights[i].count {
            neurons[i] += prev_layer.neurons[j] * weights[i][j];
        }

        neurons[i] = sigmoid(neurons[i]);
    }
}




sigmoid :: (x: float) -> float {
    ex := math.exp(x);
    return ex / (1 + ex);
}

